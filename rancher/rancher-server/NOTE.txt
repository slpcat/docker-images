多集群cluster mesh方案规划实施

1. kubernetes集群外bgp路由
  宿主机同一子网
  支持rr路由反射
  高可用需要2台或更多

  用途 打通各虚拟网络和真实网络(安装过程略)

2. rancher服务端

  独立3节点kubernetes高可用集群

3. 使用rancher安装无网络的k8s 集群

4. 网络插件 kube-router + cilium 共同实现

5. servicemesh 管理用cilium实现

   N个kubernetes集群网络互通并且组成cluster mesh

Pod IP CIDR 唯一
Node IP 唯一并且可以互访
cluser-name 唯一
cluster-id 唯一

A 安装Rancher服务端
https://rancher.com/docs/rancher/v2.x/en/quick-start-guide/deployment/quickstart-manual-setup/#1-provision-a-linux-host

docker run -d --restart=always --privileged -v /data/rancher:/var/lib/rancher/ -p 80:80 -p 443:443 -e AUDIT_LEVEL=3 rancher/rancher:v2.11.2

v2.9.2

docker run -itd --name rancher2.5.8 -p 80:80 -p 443:443 \
-v /data/rancher:/var/lib/rancher/ \
--privileged=true \
--restart=unless-stopped \
-e CATTLE_AGENT_IMAGE="registry.cn-hangzhou.aliyuncs.com/rancher/rancher-agent:v2.5.8" \
registry.cn-hangzhou.aliyuncs.com/rancher/rancher:v2.5.8

cat /opt/registries.yaml
mirrors:
  # 私有仓库域名
  harbor.kingsd.top:
    endpoint:
      - "https://harbor.kingsd.top"
configs:
  "harbor.kingsd.top":
    auth:
      username: admin  # 这是私有镜像仓库的用户名
      password: Password  # 这是私有镜像仓库的密码

-e CATTLE_SYSTEM_DEFAULT_REGISTRY=harbor.kingsd.top \ # 设置私有仓库域名
-v /opt/registries.yaml:/etc/rancher/k3s/registries.yaml \ # 将宿主机`registries.yaml`映射到容器内


购买的证书
docker run -d --restart=always --privileged --name rancher-server \
    -v /data/rancher:/var/lib/rancher \
    -p 443:443 -e AUDIT_LEVEL=3   \
    -v /data/cert/4366172_kuxxxx.rxxxox.cn.pem:/etc/rancher/ssl/cert.pem \
    -v /data/cert/4366172_kuxxxx.rxxxox.cn.key:/etc/rancher/ssl/key.pem  \
    -e CATTLE_AGENT_IMAGE="rancher/rancher-agent:v2.9.2" \
    rancher/rancher:v2.9.2 --no-cacerts

注意全局设置
agent-tls-mode: System Store

自签证书ca
docker run -d --restart=always --privileged --name rancher-server \
    -p 443:443 -e AUDIT_LEVEL=3   \
    -v /data/rancher:/var/lib/rancher \
    -v $PWD/certs/cert.pem:/etc/rancher/ssl/cert.pem \
    -v $PWD/certs/key.pem:/etc/rancher/ssl/key.pem \
    -v $PWD/certs/ca.pem:/etc/rancher/ssl/cacerts.pem \
    -e CATTLE_SYSTEM_DEFAULT_REGISTRY=harbor.example.top \   # 设置私有仓库域名
    -v /data/rancher/registries.yaml:/etc/rancher/k3s/registries.yaml \  # 将宿主机`registries.yaml`映射到容器内
    -e CATTLE_AGENT_IMAGE="rancher/rancher-agent:v2.9.2" \
    rancher/rancher:v2.9.2

docker run -d --restart=always --privileged --name rancher-server \
    -p 443:443 -e AUDIT_LEVEL=3   \
    -e CATTLE_SYSTEM_DEFAULT_REGISTRY=harbor.site.top \
    -e CATTLE_AGENT_IMAGE="harbor.site.top/rancher/rancher-agent:v2.9.2" \
    -e CATTLE_SYSTEM_CATALOG=bundled \
    -v /data/rancher/cert/rancher-ui.site.top.cer:/etc/rancher/ssl/cert.pem \
    -v /data/rancher/cert/rancher-ui.site.top.key:/etc/rancher/ssl/key.pem  \
    -v /data/rancher:/var/lib/rancher \
    -v /data/rancher/registries.yaml:/etc/rancher/k3s/registries.yaml \
    harbor.site.top/rancher/rancher:v2.9.2  --no-cacerts

高可用安装需要独立kubernetes集群和负载均衡器
helm repo add rancher-stable https://releases.rancher.com/server-charts/stable

helm fetch rancher-latest/rancher
helm fetch rancher-stable/rancher --version=v2.9.2


内核参数
systemd.unified_cgroup_hierarchy=0
systemd.legacy_systemd_cgroup_controller
https://docs.rke2.io/known_issues/

B 创建kubernetes集群

注意配置CLUSTER CIDR 和POD CIDR

1.kube-proxy 启用ipvs
services:  
  kubeproxy:
    extra_args:
      proxy-mode: ipvs

2.kubelet 允许修改sysctl

3.修改cluster/service CIDR

4.禁止自带network plugin,网络插件安装见下一步

配置样本
https://rancher.com/docs/rke/latest/en/example-yamls/

详细内容见cluster1.yml

添加node命令
sudo docker run -d --privileged --restart=unless-stopped --net=host -v /etc/kubernetes:/etc/kubernetes -v /var/run:/var/run rancher/rancher-agent:v2.5.8 --server https://192.168.159.41 --token blhlzlvfjs8scddmsnq6hh5ctffd2z9hglh9t2bxfrbdwsb286qmgf --ca-checksum d7b892e2c588dc774ab6fc1d9f6878453f9ca622d45cfe5159754df16fd029e7 --etcd --controlplane --worker

C 安装网络插件

4 禁止自带network plugin,替换为独立kube-router,仅使用bgp路由功能,其他功能禁用
https://docs.cilium.io/en/v1.10/gettingstarted/kube-router/

#安装cni
#https://github.com/containernetworking/plugins/releases
#/opt/cni/bin

#CLUSTERCIDR=10.32.0.0/12 \
#APISERVER=https://cluster01.int.domain.com:6443 \
#sh -c 'curl https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/generic-kuberouter-all-features.yaml -o - | \
#sed -e "s;%APISERVER%;$APISERVER;g" -e "s;%CLUSTERCIDR%;$CLUSTERCIDR;g"' | \
#kubectl apply -f -

启用bgp RR 反射，打通多个集群网络
https://github.com/cloudnativelabs/kube-router/blob/master/docs/bgp.md

Node-To-Node Peering Without Full Mesh
集群外部BGP RR反射器地址 192.168.159.252

#generic-kuberouter-all-features-advertise-routes-external-rr.yaml
generic-kuberouter-only-advertise-routes-external-rr-cilium.yml

kubernetes node 设置annotate
#kubectl annotate node <kube-node> "kube-router.io/peer.ips=192.168.159.252"
kubectl annotate node <kube-node> "kube-router.io/peer.asns=64514"
kubectl annotate node <kube-node> "kube-router.io/node.asn=64514"


5.禁止自带network plugin,kube-router提供bgp网络，其他功能替换为cilium v1.10 启用etcd功能,network policy

注意端口范围分配
nodeport
ephemeral port

为了达到最佳性能，建议使用IPVLAN

cat <<EOF | sudo tee /etc/systemd/system/sys-fs-bpf.mount
[Unit]
Description=Cilium BPF mounts
Documentation=http://docs.cilium.io/
DefaultDependencies=no
Before=local-fs.target umount.target
After=swap.target

[Mount]
What=bpffs
Where=/sys/fs/bpf
Type=bpf

[Install]
WantedBy=multi-user.target
EOF

#kubectl create -f https://raw.githubusercontent.com/cilium/cilium/1.6.5/install/kubernetes/quick-install.yaml

#kubectl create -f generic-kuberouter-all-features-advertise-routes.yaml

https://docs.cilium.io/en/latest/cmdref/cilium-agent/

配置样本
cilium-bgp-ipvlan-l3s-etcd-v1.7.0.yml


6.禁止自带network plugin,cilium提供网络直接路由，其他功能替换为cilium v1.7 启用etcd功能,network policy

D 使用cilium实现ClusterMesh
https://cilium.readthedocs.io/en/stable/gettingstarted/clustermesh/

#Submariner 打通多个集群 ipsec
#https://github.com/submariner-io/submariner#installation


6. 安装metallb启用L2 LoadBalancer

Expose the Cilium etcd to other clusters
cilium-etcd-external-gke.yaml

https://docs.cilium.io/en/v1.6/gettingstarted/kube-router/
https://cilium.readthedocs.io/en/stable/gettingstarted/clustermesh/

git clone https://github.com/cilium/clustermesh-tools.git
cd clustermesh-tools

todo:

删除kube-proxy
https://docs.rancher.cn/rancher2x/install-prepare/best-practices/kubernetes.html#_1-kube-apiserver

helm template cilium/cilium --version 1.7.0 \
  --namespace kube-system \
  --set global.datapathMode=ipvlan \
  --set global.ipvlan.masterDevice=eth0 \
  --set global.tunnel=disabled \
  --set global.etcd.enabled=true \
  --set global.etcd.managed=true > cilium-ipvlan.yml

helm template cilium --namespace=kube-system \
  --set global.hubble.enabled=true \
  --set global.hubble.listenAddress=":4244" \
  --set global.hubble.metrics.enabled="{dns,drop,tcp,flow,port-distribution,icmp,http}" \
  --set global.hubble.relay.enabled=true \
  --set global.hubble.ui.enabled=true > experimental-install.yaml

helm template cilium/cilium \
         --version 1.10.3 \
         --namespace kube-system \
         --set cluster-name="default01" \
         --set cluster-id="1" \
         --set ipam.mode="kubernetes" \
         --set k8s.requireIPv4PodCIDR=true \
         --set etcd.enabled=true \
         --set etcd.managed=true \
         --set etcd.image.repository="quay.io/cilium/cilium-etcd-operator" \
         --set etcd.image.tag="v2.0.7" \
         --set etcd.clusterDomain=cluster.local \
         --set operator.image.repository="quay.io/cilium/operator" \
         --set operator.image.tag="v1.10.0" \
         --set image.repository="quay.io/cilium/cilium" \
         --set image.tag="v1.10.0" \
         --set certgen.image.repository="quay.io/cilium/certgen" \
         --set certgen.image.tag="v0.1.4" \
         --set identityAllocationMode=kvstore \
         --set datapathMode=ipvlan \
         --set ipvlan.masterDevice=eth0 \
         --set tunnel=disabled \
         --set l7Proxy=false \
         --set bgp.enabled=true \
         --set bgp.announce.loadbalancerIP=true \
         --set hubble.enabled=true \
         --set hubble.ui.backend.image.repository="quay.io/cilium/hubble-ui-backend" \
         --set hubble.ui.backend.image.tag="v0.7.9@sha256:632c938ef6ff30e3a080c59b734afb1fb7493689275443faa1435f7141aabe76" \
         --set hubble.ui.frontend.image.repository="quay.io/cilium/hubble-ui" \
         --set hubble.ui.frontend.image.tag="v0.7.9@sha256:e0e461c680ccd083ac24fe4f9e19e675422485f04d8720635ec41f2ba9e5562c" \
         --set hubble.tls.enabled=false \
         --set hubble.listenAddress=":4244" \
         --set hubble.metrics.enabled="{dns,drop,tcp,flow,port-distribution,icmp,http}" \
         --set hubble.relay.enabled=true \
         --set hubble.ui.enabled=true \
         --set nodeinit.enabled=true \
         --set nodeinit.image.repository="quay.io/cilium/startup-script" \
         --set nodeinit.image.tag="62bfbe88c17778aad7bef9fa57ff9e2d4a9ba0d8" \
         --set nodeinit.restartPods=true \
	 --set bpfMasquerade=true \
         --set ipMasqAgent.enabled=false \
	 --set bpf.masquerade=true \
	 --set bpf.clockProbe=true \
	 --set bpf.waitForMount=true \
	 --set bpf.preallocateMaps=true \
	 --set bpf.tproxy=true \
	 --set bpf.hostRouting=false \
         --set enableXTSocketFallback=false \
	 --set autoDirectNodeRoutes=false \
	 --set localRedirectPolicy=true \
	 --set enableK8sEndpointSlice=true \
	 --set wellKnownIdentities.enabled=true \
	 --set sockops.enabled=true \
	 --set endpointRoutes.enabled=false \
	 --set enable-node-port=true \
	 --set hostServices.enabled=true \
	 --set nodePort.enabled=true \
	 --set hostPort.enabled=true \
	 --set kubeProxyReplacement=probe \
	 --set loadBalancer.mode=dsr \
	 --set k8sServicePort=6443 >  cilium-ipvlan.yml

helm template cilium/cilium \
         --version 1.10.0 \
         --namespace kube-system \
         --set cluster-name="default01" \
         --set cluster-id="1" \
         --set ipam.mode="kubernetes" \
         --set k8s.requireIPv4PodCIDR=true \
         --set operator.image.repository="quay.io/cilium/operator" \
         --set operator.image.tag="v1.10.0" \
         --set image.repository="quay.io/cilium/cilium" \
         --set image.tag="v1.10.0" \
         --set certgen.image.repository="quay.io/cilium/certgen" \
         --set certgen.image.tag="v0.1.4" \
         --set identityAllocationMode=crd \
         --set datapathMode=ipvlan \
         --set ipvlan.masterDevice=eth0 \
         --set ipMasqAgent.enabled=true |
         --set tunnel=disabled \
         --set l7Proxy=false \
         --set hubble.enabled=true \
         --set hubble.ui.backend.image.repository="quay.io/cilium/hubble-ui-backend" \
         --set hubble.ui.backend.image.tag="v0.7.9@sha256:632c938ef6ff30e3a080c59b734afb1fb7493689275443faa1435f7141aabe76" \
         --set hubble.ui.frontend.image.repository="quay.io/cilium/hubble-ui" \
         --set hubble.ui.frontend.image.tag="v0.7.9@sha256:e0e461c680ccd083ac24fe4f9e19e675422485f04d8720635ec41f2ba9e5562c" \
         --set hubble.tls.enabled=false \
         --set hubble.listenAddress=":4244" \
         --set hubble.metrics.enabled="{dns,drop,tcp,flow,port-distribution,icmp,http}" \
         --set hubble.relay.enabled=true \
         --set hubble.ui.enabled=true \
         --set nodeinit.enabled=true \
         --set nodeinit.image.repository="quay.io/cilium/startup-script" \
         --set nodeinit.image.tag="62bfbe88c17778aad7bef9fa57ff9e2d4a9ba0d8" \
         --set nodeinit.restartPods=true \
         --set ipMasqAgent.enabled=false \
	 --set bpf.masquerade=false \
	 --set bpf.clockProbe=true \
	 --set bpf.waitForMount=true \
	 --set bpf.preallocateMaps=true \
	 --set bpf.tproxy=true \
	 --set bpf.hostRouting=false \
         --set enableXTSocketFallback=false \
	 --set autoDirectNodeRoutes=false \
	 --set localRedirectPolicy=true \
	 --set enableK8sEndpointSlice=true \
	 --set wellKnownIdentities.enabled=true \
	 --set sockops.enabled=true \
	 --set endpointRoutes.enabled=false \
	 --set enable-node-port=true \
	 --set hostServices.enabled=true \
	 --set nodePort.enabled=true \
	 --set hostPort.enabled=true \
	 --set kubeProxyReplacement=probe \
	 --set loadBalancer.mode=dsr \
	 --set k8sServicePort=6443 >  cilium-ipvlan.yml

helm template cilium/cilium \
         --version 1.13.1 \
         --namespace kube-system \
         --set cluster.name="default01" \
         --set cluster.id="1" \
         --set ipam.mode="kubernetes" \
         --set k8s.requireIPv4PodCIDR=true \
         --set ipv4NativeRoutingCIDR="10.0.0.0/8"
         --set etcd.enabled=false \
         --set etcd.managed=false \
         --set etcd.image.repository="quay.io/cilium/cilium-etcd-operator" \
         --set etcd.image.tag="v2.0.7" \
         --set etcd.clusterDomain=cluster.local \
         --set operator.image.repository="quay.io/cilium/operator" \
         --set operator.image.tag="v1.13.1" \
         --set image.repository="quay.io/cilium/cilium" \
         --set image.tag="v1.13.1" \
         --set certgen.image.repository="quay.io/cilium/certgen" \
         --set certgen.image.tag="v0.1.4" \
         #--set identityAllocationMode=kvstore \
         --set tunnel=disabled \
         --set l7Proxy=false \
         --set bgpControlPlane.enabled=true \
         --set bpf.clockProbe=true \
         --set bpf.ctAnyMax=262144 \
         --set bpf.ctTcpMax=524288 \
         --set hubble.enabled=false \
         --set hubble.ui.backend.image.repository="quay.io/cilium/hubble-ui-backend" \
         --set hubble.ui.backend.image.tag="v0.7.9@sha256:632c938ef6ff30e3a080c59b734afb1fb7493689275443faa1435f7141aabe76" \
         --set hubble.ui.frontend.image.repository="quay.io/cilium/hubble-ui" \
         --set hubble.ui.frontend.image.tag="v0.7.9@sha256:e0e461c680ccd083ac24fe4f9e19e675422485f04d8720635ec41f2ba9e5562c" \
         --set hubble.tls.enabled=false \
         --set hubble.listenAddress=":4244" \
         --set hubble.metrics.enabled="{dns,drop,tcp,flow,port-distribution,icmp,http}" \
         --set hubble.relay.enabled=true \
         --set hubble.ui.enabled=true \
         --set nodeinit.enabled=true \
         --set nodeinit.image.repository="quay.io/cilium/startup-script" \
         --set nodeinit.image.tag="62bfbe88c17778aad7bef9fa57ff9e2d4a9ba0d8" \
         --set nodeinit.restartPods=true \
	 --set bpfMasquerade=true \
         --set ipMasqAgent.enabled=false \
	 --set bpf.masquerade=true \
	 --set bpf.clockProbe=true \
	 --set bpf.waitForMount=true \
	 --set bpf.preallocateMaps=true \
	 --set bpf.tproxy=true \
	 --set bpf.hostRouting=false \
         --set enableXTSocketFallback=false \
	 --set autoDirectNodeRoutes=false \
	 --set localRedirectPolicy=true \
	 --set enableK8sEndpointSlice=true \
	 --set wellKnownIdentities.enabled=true \
	 --set sockops.enabled=true \
	 --set endpointRoutes.enabled=false \
	 --set enable-node-port=true \
	 --set hostServices.enabled=true \
	 --set nodePort.enabled=true \
	 --set hostPort.enabled=true \
	 --set kubeProxyReplacement=probe \
	 --set loadBalancer.mode=dsr \
	 --set k8sServicePort=6443 >  cilium-bgp.yml

手动修改
  native-routing-cidr: "10.42.0.0/16"
  datapath-mode: ipvlan
  cluster-id cluster-name
       - name: CILIUM_ETCD_OPERATOR_CLUSTER_DOMAIN
          value: "pve01.local"
        - name: CILIUM_ETCD_OPERATOR_ETCD_CLUSTER_SIZE
          value: "3"
debian10  
update-alternatives --config iptables

替换kube-proxy
https://github.com/rancher/rke/issues/1432

集群配置
  kubeproxy:
    # image:
    extra_env:
      - "KUBE_PROXY_NOOP=1"
      - "PATH=/opt/custom/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
    extra_binds:
      - "/opt/custom/bin:/opt/custom/bin"

kube-proxy script as placed in /opt/custom/bin

Cilium CNI Chaining
helm install cilium cilium/cilium --version 1.7.3 \
  --namespace kube-system \
  --set global.cni.chainingMode=aws-cni \
  --set global.masquerade=false \
  --set global.tunnel=disabled \
  --set global.nodeinit.enabled=true

rancher 主机驱动
https://github.com/lnxbil/docker-machine-driver-proxmox-ve

https://github.com/AliyunContainerService/docker-machine-driver-aliyunecs

https://metallb.universe.tf/

Using BIRD to run BGP
https://docs.cilium.io/en/v1.9/gettingstarted/bird/

http://arthurchiao.art/blog/k8s-l4lb/

https://github.com/facebookincubator/katran

https://github.com/kubesphere/porter

descheduler

https://github.com/kubernetes-sigs/descheduler


